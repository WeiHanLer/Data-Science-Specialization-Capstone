---
title: "Data Science Capstone Project"
author: "Ler Wei Han"
date: "September 30, 2018"
output: html_document
---

#Milestone Report
This report is supposed to show my progress for my capstone project on Natural Language Processing. In this report, I will attempt to demosntrate that I have successfully:

1. Downloaded the data and successfully loaded it in for analysis.
2. Created a basic report of summary statistics about the data set.
3. Reported any intersting findings I have amassed so far.


##Loading the data
The necessary packages for text mining and NLP are loaded. The data that we will be using is downloaded and read in as lines.
```{r, warning=FALSE,message=FALSE}
#Loading the necessary packages
library(quanteda)
library(ggplot2)
library(stringi)
library(stopwords)
```
```{r, warning=FALSE,message=FALSE,eval=FALSE}
#Downloading the data
fileURL <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
if(!file.exists(basename(fileURL))){
    download.file(fileURL)
    unzip(basename(fileURL))
}

```
```{r, warning=FALSE,message=FALSE}
#Reading in the data. UTF-8 encoding setting to accomodate for most type of characters seen in the text.
blogs<- file("./en_US/en_US.blogs.txt")
blogs <- readLines(blogs,encoding = "UTF-8", skipNul = TRUE)


news<- file("./en_US/en_US.news.txt")
news <- readLines(news,encoding = "UTF-8", skipNul = TRUE)


twitter<- file("./en_US/en_US.twitter.txt")
twitter <- readLines(twitter,encoding = "UTF-8", skipNul = TRUE)

```

A very simple summary is done to gather basic information on the data that we are dealing with.
```{r, warning=FALSE,message=FALSE}

#Obtaining file size for separate source files

blogs.size <- paste(file.info("./en_US/en_US.blogs.txt")$size / 1024 ^ 2,"MB")
news.size <- paste(file.info("./en_US/en_US.news.txt")$size / 1024 ^ 2,"MB")
twitter.size <- paste(file.info("./en_US/en_US.twitter.txt")$size / 1024 ^ 2,"MB")


#Obtaining word count for separate source files
blogwordcount<-sum(stri_count_words(blogs))
newswordcount<-sum(stri_count_words(news))
twitterwordcount<-sum(stri_count_words(twitter))


#Obtaining number of lines for separate source files
bloglinecount<-length(blogs)
newslinecount<-length(news)
twitterlinecount<-length(twitter)

size<- c(blogs.size,news.size,twitter.size)
wordcount<-c(blogwordcount,newswordcount,twitterwordcount)
linecount<-c(bloglinecount,newslinecount,twitterlinecount)


```

A summary of the file size, word count and number of lines is produced in a table.
```{r,warning=FALSE,message=FALSE}
summarytable<-matrix(c(size,wordcount,linecount),nrow =3,byrow = FALSE)
colnames(summarytable)<- c("size","wordcount","linecount")
rownames(summarytable)<-c("blog","news","twitter")
summarytable

```


#Cleaning the Data

For the purpose of exploration, only a small subset of data is used out of the corpus.Random sampling is used to subset the required data. To resolve the issue of special and unique characters in order to use the 'tolower' function, the files are converted from UTF-8 encoding to ASCII.
```{r,warning=FALSE,message=FALSE}

#Converting from UTF-8 encoding to ASCII 
blogs<- iconv(blogs, 'UTF-8', 'ASCII')
blogs<- na.omit(blogs)

news<- iconv(news, 'UTF-8', 'ASCII')
news<- na.omit(news)


twitter<- iconv(twitter, 'UTF-8', 'ASCII')
twitter<-na.omit(twitter)

#Sampling 1000 lines from each source data
data.sample <- c(sample(blogs, 1000),
                 sample(news, 1000),
                 sample(twitter, 1000))

```
The data is organised into a corpus and cleaned with the help of the **tm** package.
The cleaning process is necessary to extract useful and meaningful contents from the text source, regardless of the text source. 
The cleaning process removes URLS, special characters, stopwords, unnecessary white spaces, punctuations, numbers as well as change all text to lower case.


More info on the **Quanteda** package and its functions can be found [here](https://cran.r-project.org/web/packages/quanteda/quanteda.pdf): 


```{r,warning=FALSE,message=FALSE}


Documentfeaturematrix<-function(x,y){
    
  dfm<-dfm(x,tolower = TRUE, stem=TRUE,  dictionary = NULL, thesaurus = NULL, removePunct=TRUE, removeNumbers=TRUE, remove_symbols=TRUE, remove_twitter=TRUE, remove_url=TRUE,ngrams=y,valuetype = c("glob", "regex", "fixed"))
	
  
  }

Top30ngrams<-function(x){
  df<-topfeatures(x,30)
  data.frame(ngram=names(df), freq=df, row.names=NULL)
}

histogram <- function(df,x_axis,title) {
    ngramchart<-df[1:30,]
  ggplot(ngramchart, aes(x=reorder(ngram, -freq),y= freq)) +
    geom_bar(stat = "identity", colour = "black")+
    labs(title=title,x = x_axis, y = "Frequency") +
    theme(axis.text.x = element_text(angle = 60, size = 12, hjust = 1)) 
  
}

```
Now, we are ready to explore our cleaned corpuscorpus<-corpus[which(is.na(corpuss))] <- "NULLVALUE"!



#Data Exploration and Basic Visualization of Results

With the help of **RWeka** package, I am able to find out the most frequently occuring one,two and three word clusters in the corpus. Structuring my findings into a data frame, I made use of the popular **ggplot** package to output my results in histograms.


1. unigram histogram

```{r,warning=FALSE,message=FALSE}
dfm1<-Documentfeaturematrix(data.sample,1L)
unigram<-Top30ngrams(dfm1)
histogram(unigram,"unigram" ,"Top 30 Unigrams")

```


2. bi-gram histogram

```{r,warning=FALSE,message=FALSE}
dfm2<-Documentfeaturematrix(data.sample,2L)
bigram<-Top30ngrams(dfm2)
histogram(bigram,"bigram" ,"Top 30 Bigrams")

```

3. tri-gram histogram

```{r,warning=FALSE,message=FALSE}
dfm3<-Documentfeaturematrix(data.sample,3L)
trigram<-Top30ngrams(dfm3)
histogram(trigram,"trigram" ,"Top 30 Trigrams")

```


Exploratory analysis complete. This concludes my milestone report.
